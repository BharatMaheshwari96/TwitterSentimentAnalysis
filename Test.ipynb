{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re, nltk\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizer(review):\n",
    "    soup = BeautifulSoup(review, 'lxml')   # removing HTML encoding such as ‘&amp’,’&quot’\n",
    "    souped = soup.get_text()\n",
    "    only_words = re.sub(\"(@[A-Za-z0-9]+)|([^A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \", souped) # removing @mentions, hashtags, urls\n",
    "\n",
    "    tokens = nltk.word_tokenize(only_words)\n",
    "    removed_letters = [word for word in tokens if len(word)>2]\n",
    "    lower_case = [l.lower() for l in removed_letters]\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_result = list(filter(lambda l: l not in stop_words, lower_case))\n",
    "\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = [wordnet_lemmatizer.lemmatize(t) for t in filtered_result]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #### Loading the saved model\n",
    "    model = joblib.load('svc.sav')\n",
    "    vocabulary_model = pd.read_csv('vocabulary_SVC.csv', header=None)\n",
    "    vocabulary_model_dict = {}\n",
    "    for i, word in enumerate(vocabulary_model[0]):\n",
    "         vocabulary_model_dict[word] = i\n",
    "    tfidf = TfidfVectorizer(sublinear_tf=True, vocabulary = vocabulary_model_dict, min_df=5, norm='l2', ngram_range=(1,3)) # min_df=5 is clever way of feature engineering\n",
    "    \n",
    "    #### Reading retrieved tweets as dataframe\n",
    "    tweet_df = pd.read_csv('test.csv', encoding = \"ISO-8859-1\")\n",
    "    pd.set_option('display.max_colwidth', -1) # Setting this so we can see the full content of cells\n",
    "    #### Normalizing retrieved tweets\n",
    "    tweet_df['normalized_tweet'] = tweet_df.tweet.apply(normalizer)\n",
    "    tweet_df = tweet_df[tweet_df['normalized_tweet'].map(len) > 0] # removing rows with normalized tweets of length 0\n",
    "    print(\"Number of tweets remaining after cleaning: \", tweet_df.normalized_tweet.shape[0])\n",
    "    print(tweet_df[['tweet','normalized_tweet']].head())\n",
    "    #### Saving cleaned tweets to csv file\n",
    "    tweet_df.drop(['tweet'], axis=1, inplace=True)\n",
    "    tweet_df.to_csv('cleaned_tweet.csv', encoding='utf-8', index=False)\n",
    "    cleaned_tweet = pd.read_csv(\"cleaned_tweet.csv\", encoding = \"ISO-8859-1\")\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    cleaned_tweet_tfidf = tfidf.fit_transform(cleaned_tweet['normalized_tweet'])\n",
    "    targets_pred = model.predict(cleaned_tweet_tfidf)\n",
    "    #### Saving predicted sentiment of tweets to csv\n",
    "    cleaned_tweet['label'] = targets_pred.reshape(-1,1)\n",
    "    cleaned_tweet.drop(['normalized_tweet'], axis=1, inplace=True)\n",
    "    cleaned_tweet.to_csv('predicted_sentiment.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets remaining after cleaning:  1953\n",
      "                                                                                                                               tweet  \\\n",
      "0  I hate the new #iphone upgrade. Won't let me download apps. #ugh #apple sucks                                                       \n",
      "1  currently shitting my fucking pants. #apple #iMac #cashmoney #raddest #swagswagswag http://instagr.am/p/UUIS0bIBZo/                 \n",
      "2  I'd like to puts some CD-ROMS on my iPad, is that possible?' â Yes, but wouldn't that block the screen?\\n                         \n",
      "3  My ipod is officially dead. I lost all my pictures and videos from the 1D and 5sos concert,and from Vet Camp #hatinglife #sobbing   \n",
      "4  Been fighting iTunes all night! I only want the music I $&@*# paid for                                                              \n",
      "\n",
      "                                                                              normalized_tweet  \n",
      "0  [hate, new, iphone, upgrade, let, download, apps, ugh, apple, suck]                          \n",
      "1  [currently, shitting, fucking, pant, apple, imac, cashmoney, raddest, swagswagswag]          \n",
      "2  [like, put, rom, ipad, possible, yes, block, screen]                                         \n",
      "3  [ipod, officially, dead, lost, picture, video, so, concert, vet, camp, hatinglife, sobbing]  \n",
      "4  [fighting, itunes, night, want, music, paid]                                                 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n"
     ]
    }
   ],
   "source": [
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
